\subsection{Player experiences of games}

Recently there has been much work into using automated methods for optimising game designs. Work by Isaksen et al. (Exploring Game Space using Survival Analysis)(Discovering unique game variants) has looked into using survival analysis to optimising parameters defining gravity and jump force in the game “Flappy Bird”. Using their technique, a simple, partially stochastic AI agent is made to play the game for a parameter set a number of times, with the distribution of the scores achieved by the agent being recorded. These score distributions can then be recorded for a number of different sets of parameter values, and these distributions used to select sets of parameters that provide gameplay fitting some criteria (with potential criteria including the difficulty of the game, and the “uniqueness” of the gameplay provided by the parameter set. This reduces the need for the game designer to explore the space of the parameter set manually, and suggests that the results of automated playtesting correlate with human players experience with the game.

As well as using automated playtesting to test parameter values, automated playtesting can also be used to assess different sets of game mechanics. Work by Nelson et al. looked into using AI players to evaluate mechanics of procedurally generated games (Rules and mechanics). In this approach it was found that the automatic generation of criteria about the playability of the game were effective when combined with a genetic programming approach for automatically generating game rule sets. In many of these approaches, the criteria used for assessing the quality of games have been relatively complex in order to try to capture the subtleties of player experience, including metrics on whether game states that appear advantageous often result in the player winning (Rules and Mechanics) or whether the player the ability to significantly impact the score they will achieve (Multi-Faceted Evolution Of Simple Arcade Games).

\subsection{AI-based game design}

The reasons behind why people enjoy particular videogames but are uninterested in others has remained elusive, despite numerous attempts to quantify them (Yannakakis, 2005 Procci, 2012 Chell, 2008). A generally agreed accepted principle is that of similarly skilled opponents, whether they are human or A.I. generated, who are challenging enough for the player to keep him focused in the game while being realistically beatable so that the player does not experience frustration or even anxiety (Ibanez \& Mata, 2011).

Human performance in relatively simple videogames that rely on reaction times has been the Learning Strategies Program, originally funded by the Defense Advanced Research Projects Agency (Donchin et al., 1989). This program gave birth to the Space Fortress game, a game that  largely resembles Asteroids (Boot, 2015). Subsequent examinations of this game showed that Attention, Working Memory and even Fluid Intelligence all highly correlated with game performance (Rabbit et al., 1989). All the aforementioned variables are highly mediated by a multitude individual differences such as gender,nationality, age, socioeconomic status, handedness, working memory and even language (Lyle, Roediger \& McCabe, 2008; Cazzato et al., 2010; Stigler, 1986; Templer \& Stephens, 2014; Greiner, Schoenfield \& Liepert, 2014).Thus the selection and development of the appropriate A.I. agents, fitted to players’ individual differences, is imperative for maximising user experience. 
